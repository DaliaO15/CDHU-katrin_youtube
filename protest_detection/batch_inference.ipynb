{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66db5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastparquet import write\n",
    "\n",
    "import os\n",
    "import glob as glob\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f16676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer(nn.Module):\n",
    "    \"\"\"Modified last layer for resnet50 for your dataset\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.fc = nn.Linear(2048, 12)  # Assuming you have 12 output classes\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def modified_resnet50():\n",
    "    # Load pretrained resnet50 with a modified last fully connected layer\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model.fc = FinalLayer()\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_load():\n",
    "    \"\"\" Loads pre-trained weights to set the protest detection model \"\"\"\n",
    "    # Load the modified ResNet-50 model\n",
    "    model = modified_resnet50()\n",
    "\n",
    "    # Load the protest prediction model (dictionary)\n",
    "    model_checkpoint = torch.load('../../protest-detection-violence-estimation/model_best.pth.tar')\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to('cuda')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d7892e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_data_patterns' from 'datasets.data_files' (/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/yolo5_transf_learn_env/lib/python3.9/site-packages/datasets/data_files.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "File \u001b[0;32m/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/yolo5_transf_learn_env/lib/python3.9/site-packages/datasets/__init__.py:31\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(platform\u001b[38;5;241m.\u001b[39mpython_version()) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.7\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportWarning\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use `datasets`, Python>=3.7 is required, and the current version of Python doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match this condition.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(pyarrow\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportWarning\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use `datasets`, the module `pyarrow>=8.0.0` is required, and the current version of `pyarrow` doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match this condition.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n",
      "File \u001b[0;32m/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/yolo5_transf_learn_env/lib/python3.9/site-packages/datasets/inspect.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_module_factory, import_main_class, load_dataset_builder, metric_module_factory\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relative_to_absolute_path\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/yolo5_transf_learn_env/lib/python3.9/site-packages/datasets/load.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetBuilder\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     DEFAULT_PATTERNS_ALL,\n\u001b[1;32m     40\u001b[0m     DataFilesDict,\n\u001b[1;32m     41\u001b[0m     DataFilesList,\n\u001b[1;32m     42\u001b[0m     EmptyDatasetError,\n\u001b[1;32m     43\u001b[0m     get_data_patterns_in_dataset_repository,\n\u001b[1;32m     44\u001b[0m     get_data_patterns_locally,\n\u001b[1;32m     45\u001b[0m     get_metadata_patterns_in_dataset_repository,\n\u001b[1;32m     46\u001b[0m     get_metadata_patterns_locally,\n\u001b[1;32m     47\u001b[0m     sanitize_patterns,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_dict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetDict, IterableDatasetDict\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_data_patterns' from 'datasets.data_files' (/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/yolo5_transf_learn_env/lib/python3.9/site-packages/datasets/data_files.py)"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fb5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_swe_path = '/zpool/beast-mirror/labour-movements-mobilisation-via-visual-means/youtube_video_frames/sweden/6Fackforbund/8fX3nxZe79U'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29b825f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m      4\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m      5\u001b[0m     ])}\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mImageFolder(test_swe_path, data_transforms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)}\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m model_load()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'predict': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])}\n",
    "\n",
    "dataset = {'predict' : datasets.ImageFolder(test_swe_path, data_transforms['predict'])}\n",
    "dataloader = {'predict': torch.utils.data.DataLoader(dataset['predict'], batch_size = 1, shuffle=False, num_workers=4)}\n",
    "\n",
    "\n",
    "model = model_load()\n",
    "\n",
    "outputs = list()\n",
    "for inputs, labels in dataloader['predict']:\n",
    "    inputs = inputs.to(device)\n",
    "    output = model(inputs)\n",
    "    #output = output.to(device)\n",
    "    #index = output.data.numpy().argmax()\n",
    "    print(output)\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"def image_transform(image):\n",
    "    # Define image transformations\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match the model's input size\n",
    "        transforms.ToTensor(),           # Convert to tensor\n",
    "    ])\n",
    "    return preprocess(image)\n",
    "    \n",
    "def protest_inference(model, path_to_img:str):\n",
    "    # Load your input image\n",
    "    image = Image.open(path_to_img)\n",
    "\n",
    "    # Preprocess the image\n",
    "    input_tensor = image_transform(image)\n",
    "\n",
    "    # Add a batch dimension (1 image)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move all to cuda \n",
    "    if torch.cuda.is_available():\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    output = output.to('cpu')\n",
    "\n",
    "    # Convert the output tensor to list\n",
    "    output_list = output[0].tolist()\n",
    "    \n",
    "    return output_list\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ba6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo5_transf_learn_env",
   "language": "python",
   "name": "yolo5_transf_learn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
